{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear regression pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP78Jr4gJKxjYypyoNyGO7J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MachineLearningWithHuman/cloud/blob/master/Linear_regression_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDbjodYsWMnd",
        "colab_type": "text"
      },
      "source": [
        "#Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yim-EgF-yUhq",
        "colab_type": "text"
      },
      "source": [
        "# <b>Assumptions in Linear Regression</b>\n",
        "\n",
        "\n",
        "*   **Linear relationship.**It is also important to check for outliers since linear regression is sensitive to outlier effects.  The linearity assumption can best be tested with scatter plots.\n",
        "![alt text](https://external-content.duckduckgo.com/iu/?u=http%3A%2F%2F2.bp.blogspot.com%2F-TcRUfR96Flw%2FUBhSO2LK9CI%2FAAAAAAAAAgY%2FI40YLjWavIs%2Fs1600%2Flinear%2Bvs%2Bnonlinear.jpg&f=1&nofb=1)\n",
        "*   **Multivariate normality.**This assumption can best be checked with a histogram or a Q-Q-Plot.  Normality can be checked with a goodness of fit test, e.g., the Kolmogorov-Smirnov test.  When the data is not normally distributed a non-linear transformation (e.g., log-transformation) might fix this issue.\n",
        "![alt text](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Ftse1.mm.bing.net%2Fth%3Fid%3DOIP.gya82KlV8aRluXMS8OCOOAAAAA%26pid%3DApi&f=1)\n",
        "*   **No or little multicollinearity.**Multicollinearity occurs when the independent variables are too highly correlated with each other.\n",
        "![alt text](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fi.stack.imgur.com%2FJfCrn.jpg&f=1&nofb=1)\n",
        "Multicollinearity may be tested with three central criteria:\n",
        "\n",
        "1.    **Correlation matrix** – when computing the matrix of Pearson’s Bivariate Correlation among all independent variables the correlation coefficients need to be smaller than 1.\n",
        "2.   **Tolerance –**the tolerance measures the influence of one independent variable on all other independent variables; the tolerance is calculated with an initial linear regression analysis.  Tolerance is defined as **T = 1 – R²** for these first step regression analysis.  With T < 0.1 there might be multicollinearity in the data and with T < 0.01 there certainly is.\n",
        "3.  **Variance Inflation Factor (VIF) –** the variance inflation factor of the linear regression is defined as VIF = 1/T. With VIF > 5 there is an indication that multicollinearity may be present; with VIF > 10 there is certainly multicollinearity among the variables.\n",
        "*   No auto-correlation\n",
        "*   Homoscedasticity (static variance)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umVR0BhEeT0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "#create data for regression\n",
        "x_values = np.arange(10)\n",
        "x_train = np.array(x_values, dtype=np.float32)\n",
        "x_train = x_train.reshape(-1, 1)\n",
        "\n",
        "y_values = [2*i + 1 for i in x_values]\n",
        "y_train = np.array(y_values, dtype=np.float32)\n",
        "y_train = y_train.reshape(-1, 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMMWBwZxng76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NkbxQnmnkrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class linearregression(torch.nn.Module):\n",
        "  def __init__(self,inputsize,outputsize):\n",
        "    super(linearregression, self).__init__()\n",
        "    self.linear=torch.nn.Linear(inputsize,outputsize)\n",
        "\n",
        "  def forward(self,x):\n",
        "    out=self.linear(x)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkHsHhSSo9Nw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputDim = 1        # takes variable 'x' \n",
        "outputDim = 1       # takes variable 'y'\n",
        "learningRate = 0.01 \n",
        "epochs = 100\n",
        "\n",
        "model = linearregression(inputDim, outputDim)\n",
        "##### For GPU #######\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irrZk7aGpGT1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = torch.nn.MSELoss() \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afc0mFAOpQTs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7e977558-80a9-451e-c319-2241e6ac9fcb"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    # Converting inputs and labels to Variable\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = Variable(torch.from_numpy(x_train).cuda())\n",
        "        labels = Variable(torch.from_numpy(y_train).cuda())\n",
        "    else:\n",
        "        inputs = Variable(torch.from_numpy(x_train))\n",
        "        labels = Variable(torch.from_numpy(y_train))\n",
        "\n",
        "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # get output from the model, given the inputs\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # get loss for the predicted output\n",
        "    loss = criterion(outputs, labels)\n",
        "    print(loss)\n",
        "    # get gradients w.r.t to parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(62.0840, grad_fn=<MseLossBackward>)\n",
            "epoch 0, loss 62.084007263183594\n",
            "tensor(10.7265, grad_fn=<MseLossBackward>)\n",
            "epoch 1, loss 10.726544380187988\n",
            "tensor(1.8539, grad_fn=<MseLossBackward>)\n",
            "epoch 2, loss 1.8538856506347656\n",
            "tensor(0.3210, grad_fn=<MseLossBackward>)\n",
            "epoch 3, loss 0.32101279497146606\n",
            "tensor(0.0562, grad_fn=<MseLossBackward>)\n",
            "epoch 4, loss 0.05618102103471756\n",
            "tensor(0.0104, grad_fn=<MseLossBackward>)\n",
            "epoch 5, loss 0.010420176200568676\n",
            "tensor(0.0025, grad_fn=<MseLossBackward>)\n",
            "epoch 6, loss 0.002506240038201213\n",
            "tensor(0.0011, grad_fn=<MseLossBackward>)\n",
            "epoch 7, loss 0.0011310502886772156\n",
            "tensor(0.0009, grad_fn=<MseLossBackward>)\n",
            "epoch 8, loss 0.0008856033673509955\n",
            "tensor(0.0008, grad_fn=<MseLossBackward>)\n",
            "epoch 9, loss 0.000835428771097213\n",
            "tensor(0.0008, grad_fn=<MseLossBackward>)\n",
            "epoch 10, loss 0.0008190653170458972\n",
            "tensor(0.0008, grad_fn=<MseLossBackward>)\n",
            "epoch 11, loss 0.000808653247077018\n",
            "tensor(0.0008, grad_fn=<MseLossBackward>)\n",
            "epoch 12, loss 0.0007993387989699841\n",
            "tensor(0.0008, grad_fn=<MseLossBackward>)\n",
            "epoch 13, loss 0.000790293444879353\n",
            "tensor(0.0008, grad_fn=<MseLossBackward>)\n",
            "epoch 14, loss 0.000781384005676955\n",
            "tensor(0.0008, grad_fn=<MseLossBackward>)\n",
            "epoch 15, loss 0.0007725834148004651\n",
            "tensor(0.0008, grad_fn=<MseLossBackward>)\n",
            "epoch 16, loss 0.0007638889946974814\n",
            "tensor(0.0008, grad_fn=<MseLossBackward>)\n",
            "epoch 17, loss 0.0007552794413641095\n",
            "tensor(0.0007, grad_fn=<MseLossBackward>)\n",
            "epoch 18, loss 0.0007467729737982154\n",
            "tensor(0.0007, grad_fn=<MseLossBackward>)\n",
            "epoch 19, loss 0.0007383602205663919\n",
            "tensor(0.0007, grad_fn=<MseLossBackward>)\n",
            "epoch 20, loss 0.0007300478173419833\n",
            "tensor(0.0007, grad_fn=<MseLossBackward>)\n",
            "epoch 21, loss 0.0007218263344839215\n",
            "tensor(0.0007, grad_fn=<MseLossBackward>)\n",
            "epoch 22, loss 0.0007136974600143731\n",
            "tensor(0.0007, grad_fn=<MseLossBackward>)\n",
            "epoch 23, loss 0.0007056555477902293\n",
            "tensor(0.0007, grad_fn=<MseLossBackward>)\n",
            "epoch 24, loss 0.0006977072916924953\n",
            "tensor(0.0007, grad_fn=<MseLossBackward>)\n",
            "epoch 25, loss 0.0006898528663441539\n",
            "tensor(0.0007, grad_fn=<MseLossBackward>)\n",
            "epoch 26, loss 0.0006820851122029126\n",
            "tensor(0.0007, grad_fn=<MseLossBackward>)\n",
            "epoch 27, loss 0.0006744015263393521\n",
            "tensor(0.0007, grad_fn=<MseLossBackward>)\n",
            "epoch 28, loss 0.0006668074638582766\n",
            "tensor(0.0007, grad_fn=<MseLossBackward>)\n",
            "epoch 29, loss 0.000659298850223422\n",
            "tensor(0.0007, grad_fn=<MseLossBackward>)\n",
            "epoch 30, loss 0.0006518780719488859\n",
            "tensor(0.0006, grad_fn=<MseLossBackward>)\n",
            "epoch 31, loss 0.0006445260951295495\n",
            "tensor(0.0006, grad_fn=<MseLossBackward>)\n",
            "epoch 32, loss 0.0006372687639668584\n",
            "tensor(0.0006, grad_fn=<MseLossBackward>)\n",
            "epoch 33, loss 0.0006300885579548776\n",
            "tensor(0.0006, grad_fn=<MseLossBackward>)\n",
            "epoch 34, loss 0.0006229906575754285\n",
            "tensor(0.0006, grad_fn=<MseLossBackward>)\n",
            "epoch 35, loss 0.0006159822223708034\n",
            "tensor(0.0006, grad_fn=<MseLossBackward>)\n",
            "epoch 36, loss 0.0006090441020205617\n",
            "tensor(0.0006, grad_fn=<MseLossBackward>)\n",
            "epoch 37, loss 0.0006021879380568862\n",
            "tensor(0.0006, grad_fn=<MseLossBackward>)\n",
            "epoch 38, loss 0.000595410936512053\n",
            "tensor(0.0006, grad_fn=<MseLossBackward>)\n",
            "epoch 39, loss 0.0005887002917006612\n",
            "tensor(0.0006, grad_fn=<MseLossBackward>)\n",
            "epoch 40, loss 0.0005820700898766518\n",
            "tensor(0.0006, grad_fn=<MseLossBackward>)\n",
            "epoch 41, loss 0.0005755117745138705\n",
            "tensor(0.0006, grad_fn=<MseLossBackward>)\n",
            "epoch 42, loss 0.0005690303514711559\n",
            "tensor(0.0006, grad_fn=<MseLossBackward>)\n",
            "epoch 43, loss 0.0005626266938634217\n",
            "tensor(0.0006, grad_fn=<MseLossBackward>)\n",
            "epoch 44, loss 0.0005562811275012791\n",
            "tensor(0.0006, grad_fn=<MseLossBackward>)\n",
            "epoch 45, loss 0.0005500215920619667\n",
            "tensor(0.0005, grad_fn=<MseLossBackward>)\n",
            "epoch 46, loss 0.0005438271909952164\n",
            "tensor(0.0005, grad_fn=<MseLossBackward>)\n",
            "epoch 47, loss 0.0005377011257223785\n",
            "tensor(0.0005, grad_fn=<MseLossBackward>)\n",
            "epoch 48, loss 0.0005316514289006591\n",
            "tensor(0.0005, grad_fn=<MseLossBackward>)\n",
            "epoch 49, loss 0.0005256555741652846\n",
            "tensor(0.0005, grad_fn=<MseLossBackward>)\n",
            "epoch 50, loss 0.0005197400460019708\n",
            "tensor(0.0005, grad_fn=<MseLossBackward>)\n",
            "epoch 51, loss 0.0005138922715559602\n",
            "tensor(0.0005, grad_fn=<MseLossBackward>)\n",
            "epoch 52, loss 0.0005080951377749443\n",
            "tensor(0.0005, grad_fn=<MseLossBackward>)\n",
            "epoch 53, loss 0.0005023739067837596\n",
            "tensor(0.0005, grad_fn=<MseLossBackward>)\n",
            "epoch 54, loss 0.0004967205459252\n",
            "tensor(0.0005, grad_fn=<MseLossBackward>)\n",
            "epoch 55, loss 0.0004911267315037549\n",
            "tensor(0.0005, grad_fn=<MseLossBackward>)\n",
            "epoch 56, loss 0.0004855958395637572\n",
            "tensor(0.0005, grad_fn=<MseLossBackward>)\n",
            "epoch 57, loss 0.00048012222396209836\n",
            "tensor(0.0005, grad_fn=<MseLossBackward>)\n",
            "epoch 58, loss 0.00047472078585997224\n",
            "tensor(0.0005, grad_fn=<MseLossBackward>)\n",
            "epoch 59, loss 0.00046936998842284083\n",
            "tensor(0.0005, grad_fn=<MseLossBackward>)\n",
            "epoch 60, loss 0.00046408409252762794\n",
            "tensor(0.0005, grad_fn=<MseLossBackward>)\n",
            "epoch 61, loss 0.0004588620213326067\n",
            "tensor(0.0005, grad_fn=<MseLossBackward>)\n",
            "epoch 62, loss 0.0004536916967481375\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
            "epoch 63, loss 0.0004485832469072193\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
            "epoch 64, loss 0.0004435224982444197\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
            "epoch 65, loss 0.0004385305510368198\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
            "epoch 66, loss 0.00043358950642868876\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
            "epoch 67, loss 0.00042870696051977575\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
            "epoch 68, loss 0.0004238818655721843\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
            "epoch 69, loss 0.00041910266736522317\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
            "epoch 70, loss 0.0004143873229622841\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
            "epoch 71, loss 0.00040972139686346054\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
            "epoch 72, loss 0.00040510622784495354\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
            "epoch 73, loss 0.0004005405935458839\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
            "epoch 74, loss 0.0003960334579460323\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
            "epoch 75, loss 0.0003915773704648018\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
            "epoch 76, loss 0.00038715358823537827\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
            "epoch 77, loss 0.00038280352600850165\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
            "epoch 78, loss 0.0003784874570555985\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
            "epoch 79, loss 0.00037422412424348295\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
            "epoch 80, loss 0.0003700109082274139\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
            "epoch 81, loss 0.000365848041838035\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
            "epoch 82, loss 0.00036172562977299094\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
            "epoch 83, loss 0.00035764925996772945\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
            "epoch 84, loss 0.00035363106871955097\n",
            "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
            "epoch 85, loss 0.00034963939106091857\n",
            "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
            "epoch 86, loss 0.0003457072307355702\n",
            "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
            "epoch 87, loss 0.0003418101405259222\n",
            "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
            "epoch 88, loss 0.00033796014031395316\n",
            "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
            "epoch 89, loss 0.00033415801590308547\n",
            "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
            "epoch 90, loss 0.0003303959092590958\n",
            "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
            "epoch 91, loss 0.00032667547930032015\n",
            "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
            "epoch 92, loss 0.00032299334998242557\n",
            "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
            "epoch 93, loss 0.0003193556913174689\n",
            "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
            "epoch 94, loss 0.0003157608152832836\n",
            "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
            "epoch 95, loss 0.00031220813980326056\n",
            "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
            "epoch 96, loss 0.0003086875076405704\n",
            "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
            "epoch 97, loss 0.0003052127722185105\n",
            "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
            "epoch 98, loss 0.00030177473672665656\n",
            "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
            "epoch 99, loss 0.000298381521133706\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBDkKlEnpVYK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "d2f99bf4-7fc0-4be6-9d26-b5a1377eec39"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "with torch.no_grad(): # we don't need gradients in the testing phase\n",
        "    if torch.cuda.is_available():\n",
        "        predicted = model(Variable(torch.from_numpy(x_train).cuda())).cpu().data.numpy()\n",
        "    else:\n",
        "        predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
        "    print(predicted)\n",
        "\n",
        "plt.clf()\n",
        "plt.plot(x_train, y_train, 'go', label='True data', alpha=0.5)\n",
        "plt.plot(x_train, predicted, '--', label='Predictions', alpha=0.5)\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.0319201]\n",
            " [ 3.0268297]\n",
            " [ 5.021739 ]\n",
            " [ 7.016649 ]\n",
            " [ 9.011559 ]\n",
            " [11.006468 ]\n",
            " [13.001378 ]\n",
            " [14.996287 ]\n",
            " [16.991196 ]\n",
            " [18.986105 ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuyEuZsLpedD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}